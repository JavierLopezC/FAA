{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMuz6soJxEmP"
      },
      "source": [
        "### **Búsqueda y Minería de Información 2021-22**\n",
        "### Universidad Autónoma de Madrid, Escuela Politécnica Superior\n",
        "### Grado en Ingeniería Informática, 4º curso\n",
        "# **Implementación de un motor de búsqueda**\n",
        "\n",
        "Fechas:\n",
        "\n",
        "* Comienzo: lunes 7 / martes 8 de febrero\n",
        "* Entrega: lunes 21 / martes 22 de febrero (14:00)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-bdu5nO581M"
      },
      "source": [
        "# Introducción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlYtzLZp6KwJ"
      },
      "source": [
        "## Objetivos\n",
        "\n",
        "Los objetivos de esta práctica son:\n",
        "\n",
        "* La iniciación a la implementación de un motor de búsqueda.\n",
        "*\tUna primera comprensión de los elementos básicos necesarios para implementar un motor completo.\n",
        "*\tLa iniciación al uso de la librería [Whoosh](https://whoosh.readthedocs.io/en/latest/intro.html) en Python para la creación y utilización de índices, funcionalidades de búsqueda en texto.\n",
        "*\tLa iniciación a la implementación de una función de ránking sencilla.\n",
        "\n",
        "Los documentos que se indexarán en esta práctica, y sobre los que se realizarán consultas de búsqueda serán documentos HTML, que deberán ser tratados para extraer y procesar el texto contenido en ellos. \n",
        "\n",
        "La práctica plantea como punto de partida una pequeña API general sencilla, que pueda implementarse de diferentes maneras, como así se hará en esta práctica y las siguientes. A modo de toma de contacto y arranque de la asignatura, en esta primera práctica se completará una implementación de la API utilizando Whoosh, con lo que resultará bastante trivial la solución (en cuanto a la cantidad de código a escribir). En la siguiente práctica el estudiante desarrollará sus propias implementaciones, sustituyendo el uso de Whoosh que vamos a hacer en esta primera práctica.\n",
        "\n",
        "En términos de operaciones propias de un motor de búsqueda, en esta práctica el estudiante se encargará fundamentalmente de:\n",
        "\n",
        "a) En el proceso de indexación: recorrer los documentos de texto de una colección dada, eliminar del contenido posibles marcas tales como html, y enviar el texto a indexar por parte de Whoosh. \n",
        "\n",
        "b) En el proceso de responder consultas: implementar una primera versión sencilla de una o dos funciones de ránking en el modelo vectorial, junto con alguna pequeña estructura auxiliar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AjDofIw6Ns6"
      },
      "source": [
        "## Material proporcionado\n",
        "\n",
        "Se proporcionan (bien en el curso de Moodle o dentro de este documento):\n",
        "\n",
        "*\tVarias clases e interfaces Python a lo largo de este *notebook*, desde las que el estudiante partirá para completar código e integrará las suyas propias. \n",
        "En particular, la función **main** implementa un programa que deberá funcionar con el código a implementar por el estudiante. Además, se proporciona una celda con código que muestra las funciones más útiles de la API de Whoosh.\n",
        "*\tUna pequeña colección <ins>docs1k.zip</ins> con aproximadamente 1.000 documentos HTML, y un pequeño fichero <ins>urls.txt</ins>. Ambas representan colecciones de prueba para depurar las implementaciones y comprobar su corrección.\n",
        "*\tUn documento de texto <ins>output.txt</ins> con la salida estándar que deberá producir la ejecución de la función main."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q9udSskn_eY7"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    This is an abstract class for the search engines\n",
        "\"\"\"\n",
        "class Searcher(ABC):\n",
        "    def __init__(self, index):\n",
        "        self.index = index\n",
        "    @abstractmethod\n",
        "    def search(self, query, cutoff):\n",
        "        \"\"\" Returns a list of documents built as a pair of path and score \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wyqSp8Qxw_2",
        "outputId": "9e14d85a-f072-4bfe-b708-84453feaefbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search results for ' probability '\n",
            "1.4883011845828815 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
            "1.4264614680779018 \t https://en.wikipedia.org/wiki/Entropy\n",
            "0.647128995820211 \t https://en.wikipedia.org/wiki/Bias\n",
            "\n",
            "Total nº of documents in the collection: 3\n",
            "Total frequency of ' probability ': 29.0\n",
            "Nº documents containing ' probability ': 3\n",
            "\tFrequency of ' probability ' in document 0 : 12\n",
            "\tFrequency of ' probability ' in document 1 : 1\n",
            "\tFrequency of ' probability ' in document 2 : 16\n",
            "Frequency of ' probability ' in document 0 https://en.wikipedia.org/wiki/Simpson's_paradox : 12\n",
            "Top 5 most frequent terms in document 0 https://en.wikipedia.org/wiki/Simpson's_paradox\n",
            "\t ('simpson', 54)\n",
            "\t ('mw', 52)\n",
            "\t ('paradox', 52)\n",
            "\t ('output', 51)\n",
            "\t ('parser', 51)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Whoosh API\n",
        "import whoosh\n",
        "from whoosh.fields import Schema, TEXT, ID\n",
        "from whoosh.formats import Format\n",
        "from whoosh.qparser import QueryParser\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import os, os.path\n",
        "import shutil\n",
        "\n",
        "Document = Schema(\n",
        "        path=ID(stored=True),\n",
        "        content=TEXT(vector=Format))\n",
        "\n",
        "def whooshexample_buildindex(dir, urls):\n",
        "    if os.path.exists(dir): shutil.rmtree(dir)\n",
        "    os.makedirs(dir)\n",
        "    writer = whoosh.index.create_in(dir, Document).writer()\n",
        "    for url in urls:\n",
        "        writer.add_document(path=url, content=BeautifulSoup(urlopen(url).read(), \"lxml\").text)\n",
        "    writer.commit()\n",
        "\n",
        "def whooshexample_search(dir, query):\n",
        "    index = whoosh.index.open_dir(dir)\n",
        "    searcher = index.searcher()\n",
        "    qparser = QueryParser(\"content\", schema=index.schema)\n",
        "    print(\"Search results for '\", query, \"'\")\n",
        "    for docid, score in searcher.search(qparser.parse(query)).items():\n",
        "        print(score, \"\\t\", index.reader().stored_fields(docid)['path'])\n",
        "    print()\n",
        "\n",
        "def whooshexample_examine(dir, term, docid, n):\n",
        "    reader = whoosh.index.open_dir(dir).reader()\n",
        "    print(\"Total nº of documents in the collection:\", reader.doc_count())\n",
        "    print(\"Total frequency of '\", term, \"':\", reader.frequency(\"content\", term))\n",
        "    print(\"Nº documents containing '\", term, \"':\", reader.doc_frequency(\"content\", term))\n",
        "    for p in reader.postings(\"content\", term).items_as(\"frequency\") if reader.doc_frequency(\"content\", term) > 0 else []:\n",
        "        print(\"\\tFrequency of '\", term, \"' in document\", p[0], \":\", p[1])\n",
        "    raw_vec = reader.vector(docid, \"content\")\n",
        "    raw_vec.skip_to(term)\n",
        "    if raw_vec.id() == term:\n",
        "        print(\"Frequency of '\", raw_vec.id(), \"' in document\", docid, reader.stored_fields(docid)['path'], \":\", raw_vec.value_as(\"frequency\"))\n",
        "    else:\n",
        "        print(\"Term '\", term, \"' not found in document\", docid)\n",
        "    print(\"Top\", n, \"most frequent terms in document\", docid, reader.stored_fields(docid)['path']) \n",
        "    vec = reader.vector(docid, \"content\").items_as(\"frequency\")\n",
        "    for p in sorted(vec, key=lambda x: x[1], reverse=True)[0:n]:\n",
        "        print(\"\\t\", p)\n",
        "    print()\n",
        "\n",
        "urls = [\"https://en.wikipedia.org/wiki/Simpson's_paradox\", \n",
        "        \"https://en.wikipedia.org/wiki/Bias\",\n",
        "        \"https://en.wikipedia.org/wiki/Entropy\"]\n",
        "\n",
        "dir = \"index/whoosh/example/urls\"\n",
        "\n",
        "whooshexample_buildindex(dir, urls)\n",
        "whooshexample_search(dir, \"probability\")\n",
        "whooshexample_examine(dir, \"probability\", 0, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ciKzD4D6Xn6"
      },
      "source": [
        "## Calificación\n",
        "\n",
        "Esta práctica se calificará con una puntuación de 0 a 10 atendiendo a las puntuaciones individuales de ejercicios y apartados dadas en el enunciado.  \n",
        "\n",
        "El peso de la nota de esta práctica en la calificación final de prácticas es del **20%**.\n",
        "\n",
        "La calificación se basará en a) el **número** de ejercicios realizados y b) la **calidad** de los mismos. \n",
        "La puntuación que se indica en cada apartado es orientativa, en principio se aplicará tal cual se refleja pero podrá matizarse por criterios de buen sentido si se da el caso.\n",
        "\n",
        "Para dar por válida la realización de un ejercicio, el código deberá funcionar (a la primera) integrado con las clases que se facilitan, **sin ninguna modificación**. El profesor comprobará este aspecto añadiendo los módulos entregados por el estudiante a los módulos facilitados en la práctica, ejecutando la función main así como otros main de prueba adicionales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnln3zQV6anE"
      },
      "source": [
        "## Entrega\n",
        "\n",
        "La entrega consistirá en un único fichero tipo *notebook* donde se incluirán todas las **implementaciones** solicitadas en cada ejercicio, así como una explicación de cada uno a modo de **memoria**.\n",
        "\n",
        "En concreto, se debe documentar:\n",
        "\n",
        "- Qué version(es) del modelo vectorial se ha(n) implementado en el ejercicio 2.\n",
        "- Cómo se ha conseguido colocar un documento en la primera posición de ránking, para cada buscador implementado en el ejercicio 2.\n",
        "- El trabajo realizado en el ejercicio 3. \n",
        "- Y cualquier otro aspecto que el estudiante considere oportuno destacar.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GtUWMA76bP0"
      },
      "source": [
        "## Indicaciones\n",
        "\n",
        "Se podrán definir clases adicionales a las que se indican en el enunciado, por ejemplo, para reutilizar código. Y el estudiante podrá utilizar o no el software que se le proporciona, con la siguiente limitación: \n",
        "\n",
        "*\tNo deberá editarse el software proporcionado más allá de donde se indica explícitamente.\n",
        "*\tEl programa **main deberá ejecutar** correctamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gICjfJ-B6g0Y"
      },
      "source": [
        "# Ejercicio 1: Implementación basada en Whoosh\n",
        "\n",
        "Implementar las clases y módulos necesarios para que el programa main funcione. Se deja al estudiante deducir alguna de las relaciones jerárquicas entre las clases Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m18qmDwAzANn"
      },
      "source": [
        "## Ejercicio 1.1: Indexación (3pt)\n",
        "\n",
        "Definir las siguientes clases:\n",
        "\n",
        "* Index.\n",
        "* WhooshIndex, como subclase de Index.\n",
        "*\tWhooshBuilder, como subclase de Builder.\n",
        "\n",
        "Se sugiere utilizar dos “fields” en el esquema de documentos de Whoosh: la ruta del documento (dirección Web o ruta en disco), y el contenido del documento.\n",
        "\n",
        "La entrada para construir el índice (método Builder.build()) podrá ser a) un fichero de texto con direcciones Web (una por línea); b) una carpeta del disco (se indexarán todos los ficheros de la carpeta, sin entrar en subcarpetas); o c) un archivo zip que contiene archivos comprimidos a indexar. Supondremos que el contenido a indexar es siempre HTML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YiNJ9ru19cN0"
      },
      "outputs": [],
      "source": [
        "class Index(ABC):\n",
        "    def __init__ (self, path):\n",
        "\n",
        "        self.path = path\n",
        "\n",
        "    @abstractmethod\n",
        "    def doc_freq(self, term: str):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def ndocs(self):\n",
        "        pass\n",
        "        #return self.documents.keys.len()\n",
        "\n",
        "    @abstractmethod\n",
        "    def doc_ids(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod     \n",
        "    def doc_terms(doc_id):\n",
        "        pass\n",
        "    @abstractmethod\n",
        "    def all_terms():\n",
        "        pass\n",
        "        #return self.terms.keys\n",
        "\n",
        "    @abstractmethod\n",
        "    def term_freq(word, doc_id):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def all_terms_with_freq():\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def doc_path(doc_id: str):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def total_freq(word):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def commit():\n",
        "        pass\n",
        "            \n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Eu23hSs6_wvX",
        "cellView": "code"
      },
      "outputs": [],
      "source": [
        "import whoosh\n",
        "from whoosh.fields import Schema, TEXT, ID\n",
        "from whoosh.formats import Format\n",
        "from whoosh.qparser import QueryParser\n",
        "import zipfile\n",
        "\n",
        "# A schema in Whoosh is the set of possible fields in a document in\n",
        "# the search space. We just define a simple 'Document' schema\n",
        "Document = Schema(\n",
        "    path=ID(stored=True),\n",
        "    content=TEXT(vector=Format))\n",
        "\n",
        "\n",
        "class Builder(ABC):\n",
        "\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "\n",
        "    @abstractmethod\n",
        "    def add_document(self, path, content):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def commit(self):\n",
        "        pass\n",
        "\n",
        "    # @abstractmethod\n",
        "    # def writer(self):\n",
        "    #     pass\n",
        "\n",
        "    def build(self, collection):\n",
        "        if os.path.isdir(collection):\n",
        "            content = os.listdir(collection)\n",
        "            for f in content:\n",
        "                file = os.path.join(collection, f)\n",
        "                with open(file) as fp:\n",
        "                    # print('Se añade el documento '+ fp.read() + 'al indice de ' + self.path)\n",
        "                    self.writer.add_document(\n",
        "                        path=file, content=fp.read())\n",
        "                    fp.close()\n",
        "                \n",
        "        elif os.path.isfile(collection):\n",
        "            if zipfile.is_zipfile(collection):\n",
        "                zip = zipfile.ZipFile(collection)\n",
        "                content = zip.namelist()\n",
        "                # print('ES UN ZIP: ')\n",
        "                # print(content)\n",
        "\n",
        "                for f in content:\n",
        "                    with zip.open(f) as fp:\n",
        "                        if f[-1] == '/':\n",
        "                            pass\n",
        "                        else:\n",
        "                            self.writer.add_document(\n",
        "                                path=f, content=BeautifulSoup(fp, \"lxml\").text)\n",
        "            elif collection.endswith('.txt'):\n",
        "                with open(collection) as fp:\n",
        "                    for url in fp:\n",
        "                        print('Se añade el documento '+ url + 'al indice de ' + self.path)\n",
        "                        self.writer.add_document(path=url, content=BeautifulSoup(\n",
        "                            urlopen(url).read(), \"lxml\").text)\n",
        "            else:\n",
        "                print(\"Error en los argumentos. Tipo de archivo no soportado\")\n",
        "\n",
        "\n",
        "class WhooshBuilder(Builder, object):\n",
        "    def __init__(self, path):\n",
        "        super().__init__(path)\n",
        "        self.writer = whoosh.index.create_in(self.path, Document).writer()\n",
        "\n",
        "    def commit(self):\n",
        "        #return WhooshIndex(self.path).commit()\n",
        "        return self.writer.commit()\n",
        "\n",
        "    def add_document(self, dir, content):\n",
        "        print('Se añade el documento '+ content + 'al indice de ' + dir)\n",
        "        if whoosh.index.exists_in(self.path) :\n",
        "            return whoosh.index.open_dir(self.path).writer().add_document(path=dir, content=content)\n",
        "        else:\n",
        "            return whoosh.index.create_in(self.path, Document).writer().add_document(path=dir, content=content)\n",
        "    \n",
        "    # def writer(self):\n",
        "    #     return whoosh.index.create_in(self.path, Document).writer()\n",
        "\n",
        "\n",
        "class WhooshIndex(Index):\n",
        "    def __init__(self, path):\n",
        "        super().__init__(path)\n",
        "        if not os.path.exists(path):\n",
        "            os.mkdir(path)\n",
        "        self.index = whoosh.index.open_dir(path)\n",
        "\n",
        "    def doc_freq(self, term: str):\n",
        "        return self.index.reader().doc_frequency(\"content\", term)\n",
        "\n",
        "    def ndocs(self):\n",
        "        return self.index.reader().doc_count()\n",
        "    \n",
        "    def doc_terms(self, docid):\n",
        "        return self.index.reader().stored_fields(docid)['path']\n",
        "\n",
        "    def doc_ids(self):\n",
        "        return self.index.reader().all_doc_ids()\n",
        "\n",
        "    def all_terms(self):\n",
        "        return list(self.index.reader().all_terms())\n",
        "        \n",
        "\n",
        "    def term_freq(self, word, doc_id):\n",
        "        for p in list(self.index.reader().postings(\"content\", word).items_as(\"frequency\")):\n",
        "            if p[0] == doc_id:\n",
        "                return p[1]\n",
        "        return 0\n",
        "\n",
        "\n",
        "    def all_terms_with_freq(self):\n",
        "        response = list()\n",
        "        for t in self.all_terms():\n",
        "            response.append((str(t[1].decode('utf-8')),str(self.doc_freq(t[1].decode('utf-8')))))\n",
        "        return response\n",
        "\n",
        "    def doc_path(self, doc_id: str):\n",
        "        return self.index.reader().stored_fields(doc_id)['path']\n",
        "\n",
        "    def total_freq(self, word):\n",
        "        return self.index.reader().frequency(\"content\", word)\n",
        "\n",
        "    def commit(self):\n",
        "        return self.index.writer().commit()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGjT-DbW-yHR"
      },
      "source": [
        "### Explicación/documentación\n",
        "\n",
        "*(por hacer)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEOpKvZi9cos"
      },
      "source": [
        "## Ejercicio 1.2: Búsqueda (1.5pt)\n",
        "\n",
        "Implementar la clase WhooshSearcher como subclase de Searcher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IGtH0CpF9owH"
      },
      "outputs": [],
      "source": [
        "class WhooshSearcher(Searcher):\n",
        "    def __init__(self, indexPath):\n",
        "        self.index = whoosh.index.open_dir(indexPath)\n",
        "\n",
        "    def search(self, query, cutoff):\n",
        "        results = list()  \n",
        "        searcher = self.index.searcher()\n",
        "        qparser = QueryParser(\"content\", schema=self.index.schema)\n",
        "        print(\"Search results for '\", query, \"'\")\n",
        "        for docid, score in searcher.search(qparser.parse(query)).items():\n",
        "            results.append([score, self.index.reader().stored_fields(docid)['path']])\n",
        "            if len(results) >= 5:\n",
        "                return results\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNDHrfjN-zTV"
      },
      "source": [
        "### Explicación/documentación\n",
        "\n",
        "*(por hacer)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH9u7bWi9pGc"
      },
      "source": [
        "# Ejercicio 2: Modelo vectorial\n",
        "\n",
        "Implementar dos modelos de ránking propios, basados en el modelo vectorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SphKCsSN9udY"
      },
      "source": [
        "## Ejercicio 2.1: Producto escalar (2pt)\n",
        "\n",
        "Implementar un modelo vectorial propio que utilice el producto escalar (sin dividir por las normas de los vectores) como función de ránking, por medio de la clase VSMDotProductSearcher, como subclase de Searcher.\n",
        "\n",
        "Este modelo hará uso de la clase Index y se podrá probar con la implementación WhooshIndex (puedes ver un ejemplo de esto en la función main).\n",
        "\n",
        "Además, la clase VSMDotProductSearcher será intercambiable con WhooshSearcher, como se puede ver en main, donde la función test_search utiliza una implementación u otra sin distinción.\n",
        "\n",
        "Para simplificar, aplicar a las consultas simplemente una separación de palabras por espacios en blanco y normalización a minúsculas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c03112f1-6337-4348-d235-81a69702610a",
        "id": "Je7VQoWWSL7T"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "    class VSMDotProductSearcher(Searcher):\n",
        "        def __init__(self, index):\n",
        "            self.index = index\n",
        "\n",
        "        def search(self, query, cutoff):\n",
        "            table = {}\n",
        "            score = {}\n",
        "            producto = 0.0\n",
        "            tokens = nltk.word_tokenize(query)\n",
        "            print(tokens)\n",
        "            for word in tokens:\n",
        "                    if((word != \"\")and (word in table)):\n",
        "                       num = table[word]\n",
        "                       table[word] = num + 1\n",
        "                    elif(word != \"\"): \n",
        "                        table[word] = 1\n",
        "\n",
        "            dic = sorted(table.items(), key = lambda asd:asd[1], reverse = True)\n",
        "            print(dic)\n",
        "            for doc in self.index.doc_ids():\n",
        "                for i in range(0, len(dic)):\n",
        "                    key = dic[i][0]\n",
        "                    value = dic[i][1]\n",
        "                    nmatches = self.index.term_freq(key, doc)\n",
        "                    producto += (nmatches * value)\n",
        "                    print(key, \"in document\", self.index.doc_path(doc), \":nmatches = \", nmatches, \"* \", value,\"= producto = \", producto)\n",
        "                score[self.index.doc_terms(doc)]= producto\n",
        "                producto = 0.0 \n",
        "            results = sorted(score.items(), key = lambda asd:asd[1], reverse = True)\n",
        "            print(\"Search results for '\", query, \"'\")\n",
        "            return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgumTPoT-1WD"
      },
      "source": [
        "### Explicación/documentación\n",
        "\n",
        "*(por hacer)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o5TvvE5-BVK"
      },
      "source": [
        "### Ejercicio\n",
        "\n",
        "Añadir a mano un documento a la colección docs1k.zip de manera que aparezca el primero para la consulta “obama family tree” para este buscador. Documentar cómo se ha conseguido y por qué resulta así.\n",
        "\n",
        "*(por hacer)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPPNbWNe-HcR"
      },
      "source": [
        "## Ejercicio 2.2: Coseno (1.5pt)\n",
        "\n",
        "Refinar la implementación del modelo para que calcule el coseno, definiendo para ello una clase VSMCosineSearcher. Para ello se necesitará extender WhooshBuilder con el cálculo de los módulos de los vectores, que deberán almacenarse en un fichero, en la carpeta de índice junto a los ficheros que genera Whoosh. \n",
        "\n",
        "Pensad en qué parte del diseño interesa hacer esto, en concreto, qué clase y en qué momento tendría que calcular, devolver y/o almacenar estos módulos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n30WQX_-RMR"
      },
      "outputs": [],
      "source": [
        "    class VSMCosineSearcher(VSMDotProductSearcher):\n",
        "        ## TODO ##\n",
        "        # Your code here #\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJqVKt6o-2oB"
      },
      "source": [
        "### Explicación/documentación\n",
        "\n",
        "*(por hacer)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh83cdIu-Re1"
      },
      "source": [
        "### Ejercicio\n",
        "\n",
        "Añadir a mano un documento a la colección docs1k.zip de manera que aparezca el primero para la consulta “obama family tree” para este buscador. Documentar cómo se ha conseguido y por qué resulta así.\n",
        "\n",
        "*(por hacer)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Wf1RFu8-V8P"
      },
      "source": [
        "# Ejercicio 3: Estadísticas de frecuencias (2pt)\n",
        "\n",
        "Utilizando las funcionalidades de la clase Index, implementar una función term_stats que calcule a) las frecuencias totales en la colección de los términos, ordenadas de mayor a menor, y b) el número de documentos que contiene cada término, igualmente de mayor a menor. Visualizar las estadísticas obtenidas en dos gráficas en escala log-log (por cada colección –seis gráficas en total), que se mostrarán en el cuaderno entregado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2EgV-4w-erd"
      },
      "outputs": [],
      "source": [
        "    def term_stats(index):\n",
        "        ## TODO ##\n",
        "        # Your code here #\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlXLFNH3-4MH"
      },
      "source": [
        "### Explicación/documentación\n",
        "\n",
        "*(por hacer)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfgNDMM6-e7k"
      },
      "source": [
        "# Programa de prueba **main**\n",
        "\n",
        "Descarga los ficheros del curso de Moodle y coloca sus contenidos en una carpeta *collections* en el mismo directorio que este *notebook*. El fichero *toy.zip* hay que descomprimirlo para indexar la carpeta que contiene."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op6MnWWE_FMj"
      },
      "source": [
        "## Función **main**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VXS8648MzPO3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5f1d34eb-5f3d-4da1-a2d8-936241a10853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "Testing indices and search on ./collections/toy/\n",
            "Building index with <class '__main__.WhooshBuilder'>\n",
            "Collection: ./collections/toy/\n",
            "Done ( 0.0074045658111572266 seconds )\n",
            "\n",
            "Reading index with <class '__main__.WhooshIndex'>\n",
            "Collection size: 4\n",
            "Vocabulary size: 43\n",
            "  Top 5 most frequent terms:\n",
            "\taa\t2=9.0\n",
            "\tbb\t2=5.0\n",
            "\tcc\t2=3.0\n",
            "\tache\t1=1.0\n",
            "\taye\t1=1.0\n",
            "[('aa', '2'), ('bb', '2'), ('cc', '2'), ('ache', '1'), ('aye', '1'), ('coil', '1'), ('come', '1'), ('consummation', '1'), ('dd', '1'), ('death', '1'), ('devoutly', '1'), ('die', '1'), ('dream', '1'), ('dreams', '1'), ('end', '1'), ('flesh', '1'), ('give', '1'), ('heart', '1'), ('heir', '1'), ('more', '1'), ('mortal', '1'), ('must', '1'), ('natural', '1'), ('no', '1'), ('off', '1'), ('opposing', '1'), ('pause', '1'), ('perchance', '1'), ('rub', '1'), ('say', '1')]\n",
            "\n",
            "  Frequency of word \"cc\" in document 0 - ./collections/toy/d3.txt: 1\n",
            "  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n",
            "  Docs containing the word'cc': 2\n",
            "Done ( 0.027878761291503906 seconds )\n",
            "\n",
            "------------------------------\n",
            "Checking search results\n",
            "  WhooshSearcher for query 'aa dd'\n",
            "Search results for ' aa dd '\n",
            "\n",
            "Done ( 0.002120494842529297 seconds )\n",
            "\n",
            "  VSMDotProductSearcher for query 'aa dd'\n",
            "['aa', 'dd']\n",
            "[('aa', 1), ('dd', 1)]\n",
            "aa in document ./collections/toy/d3.txt :nmatches =  1 *  1 = producto =  1.0\n",
            "dd in document ./collections/toy/d3.txt :nmatches =  0 *  1 = producto =  1.0\n",
            "aa in document ./collections/toy/d2.txt :nmatches =  0 *  1 = producto =  0.0\n",
            "dd in document ./collections/toy/d2.txt :nmatches =  1 *  1 = producto =  1.0\n",
            "aa in document ./collections/toy/d1.txt :nmatches =  8 *  1 = producto =  8.0\n",
            "dd in document ./collections/toy/d1.txt :nmatches =  0 *  1 = producto =  8.0\n",
            "aa in document ./collections/toy/d4.txt :nmatches =  0 *  1 = producto =  0.0\n",
            "dd in document ./collections/toy/d4.txt :nmatches =  0 *  1 = producto =  0.0\n",
            "Search results for ' aa dd '\n",
            "8.0 \t ./collections/toy/d1.txt\n",
            "1.0 \t ./collections/toy/d3.txt\n",
            "1.0 \t ./collections/toy/d2.txt\n",
            "0.0 \t ./collections/toy/d4.txt\n",
            "\n",
            "Done ( 0.04621767997741699 seconds )\n",
            "\n",
            "=================================================================\n",
            "Testing indices and search on ./collections/urls.txt\n",
            "Building index with <class '__main__.WhooshBuilder'>\n",
            "Collection: ./collections/urls.txt\n",
            "Se añade el documento https://en.wikipedia.org/wiki/Simpson's_paradox\n",
            "al indice de ./index/urls\n",
            "Se añade el documento https://en.wikipedia.org/wiki/Bias\n",
            "al indice de ./index/urls\n",
            "Se añade el documento https://en.wikipedia.org/wiki/Entropy\n",
            "al indice de ./index/urls\n",
            "Done ( 1.7850608825683594 seconds )\n",
            "\n",
            "Reading index with <class '__main__.WhooshIndex'>\n",
            "Collection size: 3\n",
            "Vocabulary size: 6412\n",
            "  Top 5 most frequent terms:\n",
            "\t0.133\t3=3.0\n",
            "\t0.1em\t3=19.0\n",
            "\t0.2em\t3=31.0\n",
            "\t0.3em\t3=11.0\n",
            "\t0.5em\t3=26.0\n",
            "[('0.133', '3'), ('0.1em', '3'), ('0.2em', '3'), ('0.3em', '3'), ('0.5em', '3'), ('01', '3'), ('10', '3'), ('10.000', '3'), ('10.1093', '3'), ('100', '3'), ('100.00', '3'), ('1000000', '3'), ('104', '3'), ('108', '3'), ('11', '3'), ('12', '3'), ('127', '3'), ('12px', '3'), ('13', '3'), ('14', '3'), ('140', '3'), ('145', '3'), ('149', '3'), ('15', '3'), ('16', '3'), ('17', '3'), ('18', '3'), ('1814400', '3'), ('19', '3'), ('1987', '3')]\n",
            "\n",
            "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox\n",
            ": 11\n",
            "  Total frequency of word \"wikipedia\" in the collection: 38.0 occurrences over 3 documents\n",
            "  Docs containing the word'wikipedia': 3\n",
            "Done ( 4.208880424499512 seconds )\n",
            "\n",
            "------------------------------\n",
            "Checking search results\n",
            "  WhooshSearcher for query 'information probability'\n",
            "Search results for ' information probability '\n",
            "https://en.wikipedia.org/wiki/Entropy\n",
            " \t 2.9381055366276314\n",
            "https://en.wikipedia.org/wiki/Simpson's_paradox\n",
            " \t 2.44657180912911\n",
            "https://en.wikipedia.org/wiki/Bias\n",
            " \t 2.0863518377684995\n",
            "\n",
            "Done ( 0.007585287094116211 seconds )\n",
            "\n",
            "  VSMDotProductSearcher for query 'information probability'\n",
            "['information', 'probability']\n",
            "[('information', 1), ('probability', 1)]\n",
            "information in document https://en.wikipedia.org/wiki/Simpson's_paradox\n",
            " :nmatches =  1 *  1 = producto =  1.0\n",
            "probability in document https://en.wikipedia.org/wiki/Simpson's_paradox\n",
            " :nmatches =  12 *  1 = producto =  13.0\n",
            "information in document https://en.wikipedia.org/wiki/Bias\n",
            " :nmatches =  16 *  1 = producto =  16.0\n",
            "probability in document https://en.wikipedia.org/wiki/Bias\n",
            " :nmatches =  1 *  1 = producto =  17.0\n",
            "information in document https://en.wikipedia.org/wiki/Entropy\n",
            " :nmatches =  43 *  1 = producto =  43.0\n",
            "probability in document https://en.wikipedia.org/wiki/Entropy\n",
            " :nmatches =  16 *  1 = producto =  59.0\n",
            "Search results for ' information probability '\n",
            "59.0 \t https://en.wikipedia.org/wiki/Entropy\n",
            "\n",
            "17.0 \t https://en.wikipedia.org/wiki/Bias\n",
            "\n",
            "13.0 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
            "\n",
            "\n",
            "Done ( 0.03425955772399902 seconds )\n",
            "\n",
            "=================================================================\n",
            "Testing indices and search on ./collections/docs1k.zip\n",
            "Building index with <class '__main__.WhooshBuilder'>\n",
            "Collection: ./collections/docs1k.zip\n",
            "Done ( 72.05639743804932 seconds )\n",
            "\n",
            "Reading index with <class '__main__.WhooshIndex'>\n",
            "Collection size: 998\n",
            "Vocabulary size: 132341\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-0633bbf7c12a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-0633bbf7c12a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \"urls\", \"wikipedia\", \"information probability\")\n\u001b[1;32m     13\u001b[0m     test_collection(collections_root_dir + \"docs1k.zip\",\n\u001b[0;32m---> 14\u001b[0;31m                     index_root_dir + \"docs\", \"seat\", \"obama family tree\")\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-0633bbf7c12a>\u001b[0m in \u001b[0;36mtest_collection\u001b[0;34m(collection_path, index_path, word, query)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# We now inspect the index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWhooshIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mtest_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-0633bbf7c12a>\u001b[0m in \u001b[0;36mtest_read\u001b[0;34m(index, word)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Collection size:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndocs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Vocabulary size:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_terms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mterms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_terms_with_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mterms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  Top 5 most frequent terms:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-dbae10a922bf>\u001b[0m in \u001b[0;36mall_terms_with_freq\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_terms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-dbae10a922bf>\u001b[0m in \u001b[0;36mdoc_freq\u001b[0;34m(self, term)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdoc_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_frequency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mndocs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/whoosh/index.py\u001b[0m in \u001b[0;36mreader\u001b[0;34m(self, reuse)\u001b[0m\n\u001b[1;32m    546\u001b[0m                 \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_toc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m                 return self._reader(self.storage, info.schema, info.segments,\n\u001b[0;32m--> 548\u001b[0;31m                                     info.generation, reuse=reuse)\n\u001b[0m\u001b[1;32m    549\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;31m# Presume that we got a \"file not found error\" because a writer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/whoosh/index.py\u001b[0m in \u001b[0;36m_reader\u001b[0;34m(cls, storage, schema, segments, generation, reuse)\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[0;31m# This index has one segment, so return a SegmentReader object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                 \u001b[0;31m# for the segment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msegreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m                 \u001b[0;31m# This index has multiple segments, so create a list of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/whoosh/index.py\u001b[0m in \u001b[0;36msegreader\u001b[0;34m(segment)\u001b[0m\n\u001b[1;32m    522\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m                     return SegmentReader(storage, schema, segment,\n\u001b[0;32m--> 524\u001b[0;31m                                          generation=generation)\n\u001b[0m\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegments\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/whoosh/reading.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, storage, schema, segment, generation, codec)\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0;31m# Get subreaders from codec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_codec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcodec\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcodec\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msegment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_codec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterms_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_perdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_codec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mper_document_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/whoosh/codec/whoosh3.py\u001b[0m in \u001b[0;36mterms_reader\u001b[0;34m(self, storage, segment)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mtiname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTERMS_EXT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mtilen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtiname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mtifile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtiname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mpostfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOSTS_EXT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/whoosh/filedb/filestore.py\u001b[0m in \u001b[0;36mopen_file\u001b[0;34m(self, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mopen_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/whoosh/filedb/compound.py\u001b[0m in \u001b[0;36mopen_file\u001b[0;34m(self, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;31m# Create a memoryview/buffer from the mmap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemoryview_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBufferFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"subset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/whoosh/filedb/structfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, buf, name, onclose)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monclose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "\n",
        "def main():\n",
        "    index_root_dir = \"./index/\"\n",
        "    collections_root_dir = \"./collections/\"\n",
        "    test_collection(collections_root_dir + \"toy/\",\n",
        "                    index_root_dir + \"toy\", \"cc\", \"aa dd\")\n",
        "    test_collection(collections_root_dir + \"urls.txt\", index_root_dir +\n",
        "                    \"urls\", \"wikipedia\", \"information probability\")\n",
        "    test_collection(collections_root_dir + \"docs1k.zip\",\n",
        "                    index_root_dir + \"docs\", \"seat\", \"obama family tree\")\n",
        "\n",
        "\n",
        "def clear(index_path: str):\n",
        "    if os.path.exists(index_path):\n",
        "        shutil.rmtree(index_path)\n",
        "    else:\n",
        "       print(\"Creating \" + index_path)\n",
        "    os.makedirs(index_path)\n",
        "\n",
        "\n",
        "def test_collection(collection_path: str, index_path: str, word: str, query: str):\n",
        "    start_time = time.time()\n",
        "    print(\"=================================================================\")\n",
        "    print(\"Testing indices and search on \" + collection_path)\n",
        "\n",
        "    # Let's create the folder if it did not exist\n",
        "    # and delete the index if it did\n",
        "    clear(index_path)\n",
        "\n",
        "    # We now test building an index\n",
        "    test_build(WhooshBuilder(index_path), collection_path)\n",
        "\n",
        "    # We now inspect the index\n",
        "    index = WhooshIndex(index_path)\n",
        "    test_read(index, word)\n",
        "\n",
        "    print(\"------------------------------\")\n",
        "    print(\"Checking search results\")\n",
        "    test_search(WhooshSearcher(index_path), query, 5)\n",
        "    test_search(VSMDotProductSearcher(WhooshIndex(index_path)), query, 5) #TODO\n",
        "    # test_search(VSMCosineSearcher(WhooshIndex(index_path)), query, 5) #TODO\n",
        "\n",
        "\n",
        "def test_build(builder, collection):\n",
        "    stamp = time.time()\n",
        "    print(\"Building index with\", type(builder))\n",
        "    print(\"Collection:\", collection)\n",
        "    # this function should index the recieved collection and add it to the index\n",
        "    builder.build(collection)\n",
        "    # when we commit, the information in the index becomes persistent\n",
        "    # we can also save any extra information we may need\n",
        "    # (and that cannot be computed until the entire collection is scanned/indexed)\n",
        "    builder.commit()\n",
        "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
        "    print()\n",
        "\n",
        "\n",
        "def test_read(index, word):\n",
        "    stamp = time.time()\n",
        "    print(\"Reading index with\", type(index))\n",
        "    print(\"Collection size:\", index.ndocs())\n",
        "    print(\"Vocabulary size:\", len(index.all_terms()))\n",
        "    terms = index.all_terms_with_freq()\n",
        "    terms.sort(key=lambda tup: tup[1], reverse=True)\n",
        "    print(\"  Top 5 most frequent terms:\")\n",
        "    for term in terms[0:5]:\n",
        "        print(\"\\t\" + term[0] + \"\\t\" + str(term[1]) +\n",
        "              \"=\" + str(index.total_freq(term)))\n",
        "    print(terms[:30])\n",
        "    # more tests\n",
        "    doc_id = 0\n",
        "    print()\n",
        "    print(\"  Frequency of word \\\"\" + word + \"\\\" in document \" + str(doc_id) +\n",
        "          \" - \" + index.doc_path(doc_id) + \": \" + str(index.term_freq(word, doc_id)))\n",
        "    print(\"  Total frequency of word \\\"\" + word + \"\\\" in the collection: \" +\n",
        "          str(index.total_freq(word)) + \" occurrences over \" + str(index.doc_freq(word)) + \" documents\")\n",
        "    print(\"  Docs containing the word'\" + word + \"':\", index.doc_freq(word))\n",
        "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
        "    print()\n",
        "\n",
        "\n",
        "def test_search(engine, query, cutoff):\n",
        "    stamp = time.time()\n",
        "    print(\"  \" + engine.__class__.__name__ + \" for query '\" + query + \"'\")\n",
        "    for path, score in engine.search(query, cutoff):\n",
        "        print(score, \"\\t\", path)\n",
        "    print()\n",
        "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
        "    print()\n",
        "\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTtUTF7j_QdF"
      },
      "source": [
        "### Salida obtenida por el estudiante\n",
        "\n",
        "*(por hacer)*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Enunciado P1",
      "provenance": [],
      "history_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}